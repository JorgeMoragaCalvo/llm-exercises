{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to come up with a joke about cats. Hmm, where do I start? Well, I know some common phrases people use for cat jokes, like \"What's the longest nose?\" but that might not be as funny or unexpected.\n",
      "\n",
      "Wait, maybe I can play on something more playful. Oh, how about mentioning things like tailoring or fashion? That could relate to cats' coats and styles. Let me think... Tailoring suggests style, and a cat's coat is tailored for its owner, so that could work.\n",
      "\n",
      "Alright, putting it together: \"Why don't cats get too much tailoring? They have the longest one in the land.\" Hmm, does that make sense? It rhymes with \"land,\" which is nice. Also, the structure is simple and relatable, which should be good for a joke.\n",
      "\n",
      "I can add an emoji to match the playful tone: ðŸ˜„ Maybe something like a cat on a tail, but since I can't use markdown here, I'll just keep it at the end of the sentence.\n",
      "\n",
      "Wait, does that flow naturally? \"Why don't cats get too much tailoring? They have the longest one in the land.\" Yeah, that seems to work. The punchline is the comparison between cats and their ability to style, which is a clever twist on the common phrase.\n",
      "</think>\n",
      "\n",
      "\"Why don't cats get too much tailoring? They have the longest one in the land! ðŸ˜„\"\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"tell me a joke about cats\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, let's break this down. The sentence is \"the cat is on the table.\" I need to translate it into Spanish.\n",
      "\n",
      "First, \"the\" translates to \"el,\" \"cat\" becomes \"cated,\" and \"on\" is \"en.\" For \"table,\" I think \"marca\" is the right word here because \"on\" was a bit tricky before. So putting it all together, I'd get \"El cated en la marca.\"\n",
      "\n",
      "Wait, let me make sure about \"marca.\" Is there another Spanish word for table? No, I don't recall any other common translations besides \"marca\" that are used interchangeably with \"table\" in this context. So, confident enough, that should be correct.\n",
      "</think>\n",
      "\n",
      "The translation of the sentence \"the cat is on the table\" into Spanish is:\n",
      "\n",
      "**El cated en la marca.**\n",
      "\n",
      "Breakdown:\n",
      "- **El** = The\n",
      "- **cated** = cat (as \"cated\")\n",
      "- **en** = in\n",
      "- **la** = the\n",
      "- **marca** = table\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Sentence: {sentence}\n",
    "Translation in {language}:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\", \"language\"])\n",
    "\n",
    "# Format the prompt with the input variables\n",
    "formatted_prompt = prompt.format(sentence=\"the cat is on the table\", language=\"spanish\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# Generate the translation using the LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- can you write a code for streamlit chatbot using ollama and langchain in python?\n",
    "- https://www.github.com/laxmimerit/ollama-chatbot\n",
    "- test llama3:2:3b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
